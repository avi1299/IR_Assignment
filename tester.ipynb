{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "f22dba344221ac07cd70860863354c1185255ecb1d98f9ff8d230ee6b60a136f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_parser(location):\n",
    "    #Creating a list of document ids\n",
    "    doc_no=[]\n",
    "    #Creating a list of words in the documents\n",
    "    words=[]\n",
    "    #Creating a list of words in the document zones i.e headings\n",
    "    zone_words=[]\n",
    "\n",
    "    #Stores the document id and it's corresponding zone i.e heading\n",
    "    zone={}\n",
    "\n",
    "    #Stores the document id and corresponding tokenised words of the document\n",
    "    tokenised={}\n",
    "\n",
    "    #Stores the document id and corresponding tokenised words of the document zone\n",
    "    zone_tokenised={}\n",
    "\n",
    "    ################################################################################\n",
    "    #Opening the corpus and reading the file\n",
    "    f=open(location, 'r' , encoding='utf8')\n",
    "    content = f.read()\n",
    "    content=str(content)\n",
    "    ################################################################################\n",
    "    #Removing <a>...</a> tags\n",
    "    pattern = re.compile(\"<(/)?a[^>]*>\")\n",
    "    content_new = re.sub(pattern,\"\", content)\n",
    "    ################################################################################\n",
    "    #Creating a folder to hold the seperated documents\n",
    "    if not os.path.exists(\"./Documents\") :\n",
    "        os.mkdir (\"./Documents\")\n",
    "\n",
    "    #Creating the folder to store dictionaries as pickle files\n",
    "    if not os.path.exists(\"./Storage\") :\n",
    "        os.mkdir (\"./Storage\")\n",
    "    ################################################################################\n",
    "\n",
    "    #Creating a soup using a html parser and iterating through each 'doc'\n",
    "    soup=BeautifulSoup(content_new,'html.parser')\n",
    "    for doc in soup.findAll('doc'):\n",
    "        #Opening a file to write the contents of the doc\n",
    "        o=open('./Documents/'+str(doc['id'])+\".txt\",'w', encoding='utf8')\n",
    "\n",
    "        #Adding the document id to doc_no and extracting the text in that doc\n",
    "        doc_no=doc_no+[(int(doc['id']))]\n",
    "        text=doc.get_text()\n",
    "\n",
    "        #Writing the text and closing the file\n",
    "        o.write(doc.get_text())\n",
    "        o.close()\n",
    "\n",
    "        #Storing the heading of the document in the dictionary called 'zone'\n",
    "        zone[int(doc['id'])]=str(text).partition('\\n\\n')[0][1:]\n",
    "\n",
    "        #Extracting the heading of the document\n",
    "        zone_text=zone[int(doc['id'])]\n",
    "\n",
    "        #Making all the text lowercase\n",
    "        text=text.lower()\n",
    "        zone_text=zone_text.lower()\n",
    "\n",
    "        #Replaces punctuations with spaces\n",
    "        text=text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "        zone_text=zone_text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "\n",
    "        #Removes weird punctuations. Add a sapce and symbol you want to replace respectively\n",
    "        text=text.translate(str.maketrans(\"‘’’–——−\",'       '))\n",
    "        zone_text=zone_text.translate(str.maketrans(\"‘’’–——−\",'       '))\n",
    "\n",
    "        #Tokeinzing word from the doc and adding it to 'words' dictionary \n",
    "        words=words+word_tokenize(text)\n",
    "        zone_words=zone_words+word_tokenize(zone_text)\n",
    "\n",
    "        #Adding the token stream to a dictionary indexed by doc_id\n",
    "        tokenised[int(doc['id'])]=word_tokenize(text)\n",
    "        zone_tokenised[int(doc['id'])]=word_tokenize(zone_text)\n",
    "        \n",
    "        #Eliminating the duplicate words\n",
    "        words=list(set(words))\n",
    "        zone_words=list(set(zone_words))\n",
    "\n",
    "        #Printing progress of processing documents\n",
    "        print(\"\\r\"+\"Parsing Progress: Document_id = \"+doc['id']+\" : \"+ zone[int(doc['id'])],end='')\n",
    "    f.close()\n",
    "\n",
    "    zone_file = open('./Storage/zone.pkl', 'wb') \n",
    "    pickle.dump(zone, zone_file) \n",
    "    zone_file.close()\n",
    "\n",
    "    doc_no_file = open('./Storage/doc_no.pkl', 'wb') \n",
    "    pickle.dump(doc_no, doc_no_file) \n",
    "    doc_no_file.close()\n",
    "\n",
    "    words_file = open('./Storage/words.pkl', 'wb') \n",
    "    pickle.dump(words, words_file) \n",
    "    words_file.close()\n",
    "\n",
    "    zone_words_file = open('./Storage/zone_words.pkl', 'wb') \n",
    "    pickle.dump(zone_words, zone_words_file) \n",
    "    zone_words_file.close()\n",
    "\n",
    "    zone_file = open('./Storage/zone.pkl', 'wb') \n",
    "    pickle.dump(zone, zone_file) \n",
    "    zone_file.close()\n",
    "\n",
    "    tokeinsed_file = open('./Storage/tokeinsed.pkl', 'wb') \n",
    "    pickle.dump(tokenised, tokeinsed_file) \n",
    "    tokeinsed_file.close()\n",
    "\n",
    "    zone_tokeinsed_file = open('./Storage/zone_tokeinsed.pkl', 'wb') \n",
    "    pickle.dump(zone_tokenised, zone_tokeinsed_file) \n",
    "    zone_tokeinsed_file.close()\n",
    "    print(\"\\nDocuments seperated and parsed\")\n",
    "    ################################################################################\n",
    "\n",
    "    #Creating empty dataframe\n",
    "    df=pd.DataFrame(0,index=doc_no,columns=words)\n",
    "    zone_df=pd.DataFrame(0,index=doc_no,columns=zone_words)\n",
    "    ################################################################################\n",
    "\n",
    "    #Populating Document-Term Frequency Table\n",
    "    for doc_id,tokenstream in tokenised.items():\n",
    "        print(\"\\r\"+\"Populating Document-Term Frequency Table with doc \"+str(doc_id),end=\"\")\n",
    "        for token in tokenstream:\n",
    "            df[token].loc[doc_id]+=1\n",
    "\n",
    "    df.to_pickle('./Storage/df.pkl','bz2')\n",
    "\n",
    "    #Populating Zone-Term Frequency Table\n",
    "    for doc_id,tokenstream in zone_tokenised.items():\n",
    "        print(\"\\r\"+\"Populating Zone-Term Frequency Table with doc \"+str(doc_id),end=\"\")\n",
    "        for token in tokenstream:\n",
    "            zone_df[token].loc[doc_id]+=1\n",
    "\n",
    "    zone_df.to_pickle('./Storage/zone_df.pkl','bz2')\n",
    "    print(\"\\nPopulating Term-Frequency Table done\")\n",
    "    ################################################################################\n",
    "    #Constructing a dictionary cotaining the term and it's inverse document frequency. Formula: idf=log(N/tf)\n",
    "    inv_doc_freq={}\n",
    "    no_of_docs=len(doc_no)\n",
    "    for word in words:\n",
    "        inv_doc_freq[word]=np.log10(no_of_docs/sum(df[word]>0))\n",
    "\n",
    "    inv_doc_freq_file = open('./Storage/inv_doc_freq.pkl', 'wb') \n",
    "    pickle.dump(inv_doc_freq, inv_doc_freq_file) \n",
    "    inv_doc_freq_file.close()\n",
    "    ################################################################################\n",
    "    #Creating and population a dictionary containg the vector of the documents\n",
    "    doc_vec={}\n",
    "    for doc_id in doc_no:\n",
    "        #Creating a vector for each document\n",
    "        vec=(1+np.log10(np.array(df.loc[doc_id]))) #*list(doc_freq.values())\n",
    "        #Replacing all -inf values with zeros. -inf reached when we take log of 0\n",
    "        vec[vec==-np.inf]=0\n",
    "        #Normalizing the vector\n",
    "        vec=vec/(np.sqrt(sum(vec**2)))\n",
    "        #Storing the vector\n",
    "        doc_vec[doc_id]=vec\n",
    "        print(\"\\r\"+\"Document Vector created for doc_no:\"+str(doc_id),end=\"\")\n",
    "    \n",
    "    doc_vec_file = bz2.BZ2File('./Storage/doc_vec.pkl', 'w')\n",
    "    pickle.dump(doc_vec, doc_vec_file) \n",
    "    doc_vec_file.close()\n",
    "\n",
    "\n",
    "    #Creating and population a dictionary containg the vector of the documents\n",
    "    zone_vec={}\n",
    "    for doc_id in doc_no:\n",
    "        #Creating a vector for each document\n",
    "        vec=(1+np.log10(np.array(zone_df.loc[doc_id])))#*list(doc_freq.values())\n",
    "        #Replacing all -inf values with zeros. -inf reached when we take log of 0\n",
    "        vec[vec==-np.inf]=0\n",
    "        #Normalizing the vector\n",
    "        vec=vec/(np.sqrt(sum(vec**2)))\n",
    "        #Storing the vector\n",
    "        zone_vec[doc_id]=vec\n",
    "        print(\"\\r\"+\"Zone Vector created for doc_no:\"+str(doc_id),end=\"\")\n",
    "\n",
    "    zone_vec_file = open('./Storage/zone_vec.pkl', 'wb') \n",
    "    pickle.dump(zone_vec, zone_vec_file) \n",
    "    zone_vec_file.close()\n",
    "    print(\"\\nDocument vector creation done\")\n",
    "    ################################################################################  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parsing Progress: Document_id = 1526 : Abner\n",
      "Documents seperated and parsed\n",
      "Populating Zone-Term Frequency Table with doc 1526\n",
      "Populating Term-Frequency Table done\n",
      "Zone Vector created for doc_no:1526\n",
      "Document vector creation done\n"
     ]
    }
   ],
   "source": [
    "location='./Text_corpus/wiki_00'\n",
    "corpus_parser(location)"
   ]
  }
 ]
}