{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('venv')",
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "f22dba344221ac07cd70860863354c1185255ecb1d98f9ff8d230ee6b60a136f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of document ids\n",
    "doc_no=[]\n",
    "#Creating a list of words in the documents\n",
    "words=[]\n",
    "\n",
    "diction={}\n",
    "\n",
    "#Opening the corpus and reading the file\n",
    "f=open('./Text_corpus/wiki_00', 'r' , encoding='utf8')\n",
    "content = f.read()\n",
    "content=str(content)\n",
    "\n",
    "#Removing <a>...</a> tags\n",
    "pattern = re.compile(\"<(/)?a[^>]*>\")\n",
    "content_new = re.sub(pattern,\"\", content)\n",
    "\n",
    "#Creating a folder to hold the seperated documents\n",
    "if not os.path.exists(\"./Documents\") :\n",
    "    os.mkdir (\"./Documents\")\n",
    "\n",
    "#Creating a soup using a html parser and iterating through each 'doc'\n",
    "soup=BeautifulSoup(content_new,'html.parser')\n",
    "for doc in soup.findAll('doc'):\n",
    "    #Opening a file to write the contents of the doc\n",
    "    o=open('./Documents/'+str(doc['id'])+\".txt\",'w', encoding='utf8')\n",
    "    doc_no=doc_no+[(int(doc['id']))]\n",
    "    text=doc.get_text()\n",
    "\n",
    "    #Making all the text lowercase\n",
    "    text=text.lower()\n",
    "\n",
    "    #Replaces punctuations with spaces\n",
    "    text=text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    #Removes weird punctuations. Add a sapce and symbol you want to replace respectively\n",
    "    text=text.translate(str.maketrans(\"‘’’–——−\",'       '))\n",
    "\n",
    "    #Tokeinzing word from the doc and adding it to 'words' dictionary \n",
    "    words=words+word_tokenize(text)\n",
    "\n",
    "    #Adding the token stream to a dictionary indexed by doc_id\n",
    "    diction[int(doc['id'])]=word_tokenize(text)\n",
    "    \n",
    "    #Eliminating the duplicate words\n",
    "    words=list(set(words))\n",
    "\n",
    "    #Writing the text and closing the file\n",
    "    o.write(doc.get_text())\n",
    "    o.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating empty dataframe\n",
    "df=pd.DataFrame(0,index=doc_no,columns=words)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populating Document-Term Frequency Table\n",
    "for doc_id,tokenstream in diction.items():\n",
    "    print(\"Populating Document-Term Frequency Table with doc \"+str(doc_id))\n",
    "    for token in tokenstream:\n",
    "        df[token].loc[doc_id]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['anarchism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Document Frequency dictionary\n",
    "doc_freq={}\n",
    "no_of_docs=len(doc_no)\n",
    "for word in words:\n",
    "    doc_freq[word]=np.log10(no_of_docs/sum(df[word]>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing out Document Frequency\n",
    "print(doc_freq['a'])\n",
    "print(doc_freq['ner'])\n",
    "print(sum(df['ner']>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Creating and population a dictionary containg the vector of the documents\n",
    "doc_vec={}\n",
    "for doc_id in doc_no:\n",
    "    #Creating a vector for each document\n",
    "    vec=(1+np.log10(np.array(df.loc[doc_id])))*list(doc_freq.values())\n",
    "    #Replacing all -inf values with zeros. -inf reached when we take log of 0\n",
    "    vec[vec==-np.inf]=0\n",
    "    #Normalizing the vector\n",
    "    vec=vec/(np.sqrt(sum(vec**2)))\n",
    "    #Storing the vector\n",
    "    doc_vec[doc_id]=vec\n",
    "print(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the dictionaries in pickle files\n",
    "if not os.path.exists(\"./Storage\") :\n",
    "    os.mkdir (\"./Storage\")\n",
    "\n",
    "doc_vec_file = open('./Storage/doc_vec.pkl', 'wb') \n",
    "pickle.dump(doc_vec, doc_vec_file) \n",
    "doc_vec_file.close()\n",
    "\n",
    "doc_freq_file = open('./Storage/doc_freq.pkl', 'wb') \n",
    "pickle.dump(doc_freq, doc_freq_file) \n",
    "doc_freq_file.close()\n",
    "\n",
    "doc_no_file = open('./Storage/doc_no.pkl', 'wb') \n",
    "pickle.dump(doc_no, doc_no_file) \n",
    "doc_no_file.close()\n",
    "\n",
    "words_file = open('./Storage/words.pkl', 'wb') \n",
    "pickle.dump(words, words_file) \n",
    "words_file.close()\n",
    "\n",
    "diction_file = open('./Storage/diction.pkl', 'wb') \n",
    "pickle.dump(diction, diction_file) \n",
    "diction_file.close()\n",
    "\n",
    "doc_vec_file = open('./Storage/doc_vec.pkl', 'wb') \n",
    "pickle.dump(doc_vec, doc_vec_file) \n",
    "doc_vec_file.close()\n",
    "\n",
    "df.to_pickle('./Storage/df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}