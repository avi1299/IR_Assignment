{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet\n",
    "#### Sources\n",
    "* [Blog](https://blog.xrds.acm.org/2017/07/power-wordnet-use-python/#:~:text=WordNet%20is%20a%20database%20of,and%20sat%20on%20the%20sofa.%E2%80%9D)\n",
    "* [Theory](https://wordnet.princeton.edu/)\n",
    "* [Medium article on query relaxation](https://queryunderstanding.com/query-relaxation-342bc37ad425)\n",
    "* [ Paper: Similarity Search Combining Query Relaxation and Diversification](https://arxiv.org/ftp/arxiv/papers/1611/1611.04689.pdf)\n",
    "* [Similarity application example](https://stackoverflow.com/questions/15730473/wordnet-find-synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import itertools\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = wn.synsets('mercator')\n",
    "A[0].hypernyms()\n",
    "#A[0].hypernyms()[0].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['faculty', 'mental_faculty', 'module']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0].hypernyms()[0].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fender', 'buffer', 'cowcatcher', 'pilot']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('fender')[1].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hypernym(query_term):\n",
    "    hypernym_list = {}\n",
    "    contexts = {}\n",
    "    for i, element in enumerate(wn.synsets(query_term)):\n",
    "        temp = element.hypernyms()[0].lemma_names()\n",
    "        contexts[i] = temp \n",
    "    return contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['nymphalid',\n",
       "  'nymphalid_butterfly',\n",
       "  'brush-footed_butterfly',\n",
       "  'four-footed_butterfly'],\n",
       " 1: ['peafowl', 'bird_of_Juno']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_hypernym('peacock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_synonyms(query_term):\n",
    "    temp = {}\n",
    "    contexts = {}\n",
    "    for i, element in enumerate(wn.synsets(query_term)):\n",
    "        temp = element.lemma_names()\n",
    "        contexts[i] = temp\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['blare', 'blaring', 'cacophony', 'clamor', 'din'], 1: ['cacophony']}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_synonyms(\"cacophony\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold =0.1\n",
    "idf=pd.read_pickle(r'./Storage/inv_doc_freq.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6483600109809315\n"
     ]
    }
   ],
   "source": [
    "print(max(idf.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type the query:top monument\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['top', 'monument']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input('Type the query:')\n",
    "query = query.lower()\n",
    "query=query.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "query=query.translate(str.maketrans(\"‘’’–——−\",'       '))\n",
    "query_words = []\n",
    "query_words = word_tokenize(query)\n",
    "query_words=list(set(query_words))\n",
    "query_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordnetImprovement:\n",
    "    def __init__(self, query_term):\n",
    "        self.term = query_term\n",
    "        \n",
    "        \n",
    "    def extract_hypernyms(self):\n",
    "        hypernym_list = {}\n",
    "        contexts = {}\n",
    "        for i, element in enumerate(wn.synsets(self.term)):\n",
    "            temp = element.hypernyms()[0].lemma_names()\n",
    "            contexts[i] = temp \n",
    "        return contexts\n",
    "\n",
    "\n",
    "    def extract_synonyms(self):\n",
    "        temp = {}\n",
    "        contexts = {}\n",
    "        for i, element in enumerate(wn.synsets(self.term)):\n",
    "            temp = element.lemma_names()\n",
    "            contexts[i] = temp\n",
    "        return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "alt_query_dict={}\n",
    "\n",
    "for token in query_words:\n",
    "    syn ={}\n",
    "    hyp = {}\n",
    "    if idf.get(token) != None and idf[token]>threshold:\n",
    "        hyp = WordnetImprovement.extract_hypernyms(token)[0]\n",
    "        syn = WordnetImprovement.extract_synonyms(token)[0]\n",
    "\n",
    "for n in range(len(syn)):\n",
    "    temp = query_words.copy()\n",
    "    for i in range(len(temp)):\n",
    "        if idf[temp[i]]>threshold:\n",
    "            idx = i\n",
    "    if temp[idx] != hyp[n]:\n",
    "        temp[idx] = hyp[n]\n",
    "        alt_query_dict[n] = temp\n",
    "\n",
    "alt_query_dict\n",
    "'''\n",
    "print('hmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_relaxation(tokenized_query, mode = 'hypernym'):\n",
    "    threshold = 1\n",
    "    parallel_query_dict = {}\n",
    "    syn = {}\n",
    "    hyp = {}\n",
    "    temp = []\n",
    "    for token in tokenized_query:\n",
    "        if idf.get(token) != None and idf[token] > threshold:\n",
    "            relaxer = WordnetImprovement(token)\n",
    "            hyp = relaxer.extract_hypernyms()[0]\n",
    "            syn = relaxer.extract_synonyms()[0]\n",
    "            \n",
    "            \n",
    "    if mode == 'hypernym':\n",
    "        for n in range(len(hyp)):\n",
    "            temp = tokenized_query.copy()\n",
    "            for i in range(len(temp)):\n",
    "                if idf[temp[i]]>threshold:\n",
    "                    idx = i\n",
    "            if temp[idx] != hyp[n]:\n",
    "                temp[idx] = hyp[n]\n",
    "                parallel_query_dict[n] = temp\n",
    "                \n",
    "                \n",
    "    elif mode == 'synonym':\n",
    "        for n in range(len(syn)):\n",
    "            temp = tokenized_query.copy()\n",
    "            for i in range(len(temp)):\n",
    "                if idf[temp[i]]>threshold:\n",
    "                    idx = i\n",
    "            if temp[idx] != syn[n]:\n",
    "                temp[idx] = syn[n]\n",
    "                parallel_query_dict[n] = temp\n",
    "    return parallel_query_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['top', 'structure'], 1: ['top', 'construction']}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_queries = query_relaxation(query_words)\n",
    "parallel_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_tester' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-2577fe475b41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mparallel_query_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparallel_queries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mparallel_query_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_tester\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_relevant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mparllel_query_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'query_tester' is not defined"
     ]
    }
   ],
   "source": [
    "parallel_query_scores = {}\n",
    "for n, query in parallel_queries.items():\n",
    "    parallel_query_scores[n] = query_tester.find_relevant(query)\n",
    "parllel_query_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
