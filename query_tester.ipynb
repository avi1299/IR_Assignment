{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import itertools\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the stored files\n",
    "vocab = pd.read_pickle(r'./Storage/words.pkl')\n",
    "inv_doc_freq=pd.read_pickle(r'./Storage/inv_doc_freq.pkl')\n",
    "doc_vector = pd.read_pickle('./Storage/doc_vec.pkl','bz2')\n",
    "zone = pd.read_pickle(r'./Storage/zone.pkl')\n",
    "zone_vec= pd.read_pickle(r'./Storage/zone_vec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating the dataframe to store our query vector and zone vector\n",
    "buffer = pd.read_pickle('./Storage/df.pkl','bz2')\n",
    "buffer.drop(buffer.index, inplace=True)\n",
    "buffer.loc[0]=0\n",
    "zone_buffer = pd.read_pickle('./Storage/zone_df.pkl','bz2')\n",
    "zone_buffer.drop(zone_buffer.index, inplace=True)\n",
    "zone_buffer.loc[0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant(query,open_web):\n",
    "    # Preprocessing the query to remove punctuations\n",
    "    query = query.lower()\n",
    "    query=query.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    query=query.translate(str.maketrans(\"‘’’–——−\",'       '))\n",
    "\n",
    "    # Tokenizing the query\n",
    "    query_words = []\n",
    "    query_words = word_tokenize(query)\n",
    "    query_words=list(set(query_words))\n",
    "    ################################################################################\n",
    "\n",
    "    #Resetting buffer and zone_buffer\n",
    "    buffer.loc[0]=0\n",
    "    zone_buffer.loc[0]=0\n",
    "    ################################################################################\n",
    "    # Populating the query term frequncy dataframe\n",
    "    threshold=0.1 #This is the idf below which which do not want to consider the words. Removes very frequent words from the zone.\n",
    "    for token in query_words:\n",
    "        buffer[token]+=1\n",
    "        if (token in zone_buffer.columns and inv_doc_freq[token]>threshold):\n",
    "            zone_buffer[token]+= inv_doc_freq[token]\n",
    "    ################################################################################\n",
    "    # Vectorizing the query doc frequnecy and calcualting weights\n",
    "    query_vec=(1+np.log10(np.array(buffer.loc[0])))*list(inv_doc_freq.values())\n",
    "    query_vec[query_vec==-np.inf]=0\n",
    "    query_vec=query_vec/(np.sqrt(sum(query_vec**2)))\n",
    "    # Convering NaN values to zero\n",
    "    query_vec = np.nan_to_num(query_vec)\n",
    "\n",
    "    # Vectorizing the query zone doc frequnecy and calcualting weights\n",
    "    zone_query_vec=np.array(zone_buffer.loc[0])\n",
    "    zone_query_vec=zone_query_vec/(np.sqrt(sum(zone_query_vec**2)))\n",
    "    zone_query_vec = np.nan_to_num(zone_query_vec)\n",
    "    ################################################################################\n",
    "    # Computing scores for the query vector corresponding to each document\n",
    "    scores = {}\n",
    "    for doc_id, sub_vector in doc_vector.items():\n",
    "        scores[doc_id] = np.sum(np.multiply(query_vec, sub_vector))\n",
    "    #maxval stores the highest score recorded for document content matching\n",
    "    #We are adding extra score if the title also matches\n",
    "    maxval=max(scores.values())\n",
    "    for doc_id, sub_vector in zone_vec.items():\n",
    "        scores[doc_id] += np.sum(np.multiply(zone_query_vec, sub_vector))*maxval\n",
    "    ################################################################################\n",
    "    # Sorting scores in descending order\n",
    "    sorted_scores = dict(sorted(scores.items(), key= operator.itemgetter(1), reverse=True))\n",
    "    # Returning the top 10 results \n",
    "    return_docs = itertools.islice(sorted_scores.items(), 10)\n",
    "    for k, v in return_docs:\n",
    "        print(k,round(v,3),zone[k])\n",
    "        #Opening the webpages in a browser for easy checking\n",
    "        if(open_web):\n",
    "            webbrowser.open('https://en.wikipedia.org/wiki?curid='+str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "615 0.181 American Football Conference\n",
      "925 0.107 Asociación Alumni\n",
      "1110 0.079 Demographics of American Samoa\n",
      "1109 0.069 Geography of American Samoa\n",
      "966 0.061 American shot\n",
      "1111 0.056 Politics of American Samoa\n",
      "1241 0.049 American (word)\n",
      "951 0.048 Antigua and Barbuda\n",
      "659 0.045 American National Standards Institute\n",
      "600 0.043 Andorra\n"
     ]
    }
   ],
   "source": [
    "# Taking query input\n",
    "query = input(\"Type the query: \")\n",
    "find_relevant(query,open_web=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "f22dba344221ac07cd70860863354c1185255ecb1d98f9ff8d230ee6b60a136f"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}